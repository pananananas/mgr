\section*{Streszczenie}

Synteza nowych widoków obiektów na podstawie pojedynczych obrazów pozostaje jednym z najbardziej wymagających problemów w widzeniu komputerowym. Istniejące metody oparte na modelach dyfuzyjnych borykają się z kompromisem między efektywnością obliczeniową a jakością wyników. Niniejsza praca wprowadza nowatorskie podejście łączące Feature-wise Linear Modulation (FiLM) do warunkowania parametrami kamery oraz równoległe adaptery atencji krzyżowej dla warunkowania widokiem referencyjnym.
Proponowana metoda trenuje jedynie $585M$ z $2.9B$ parametrów ($20\%$) przy zachowaniu porównywalnej wydajności z pełnym dostrajaniem modelu, osiągając $4$-krotnie szybsze uczenie. Walidacja na zbiorach danych ObjaverseXL \cite{objaversexl} i Google Scanned Objects \cite{gso} wykazuje korzyści w zakresie efektywności obliczeniowej w porównaniu z najlepszymi metodami Zero123++ \cite{zero1to3} i MVAdapter \cite{mvadapter}, choć z niższą jakością wyników wynikającą z ograniczonej skali danych treningowych.

\section*{Abstract}

Novel view synthesis of objects from single images remains one of the most challenging problems in computer vision. Existing diffusion-based methods face a trade-off between computational efficiency and result quality. This work introduces a novel approach combining Feature-wise Linear Modulation (FiLM) for camera parameter conditioning with parallel cross-attention adapters for visual conditioning.
The proposed method trains only $585M$ of $2.9B$ parameters ($20\%$) while achieving comparable performance to full model fine-tuning, with $4\times$ faster training. Validation on ObjaverseXL \cite{objaversexl} and Google Scanned Objects \cite{gso} datasets demonstrates computational efficiency benefits compared to state-of-the-art Zero123++ \cite{zero1to3} and MVAdapter \cite{mvadapter} methods, though with performance gaps attributable to constrained training data scale.
