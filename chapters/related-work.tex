\Chapter{Related Work}\label{chapter:related}

In this chapter, I provide a comprehensive review of existing
approaches relevant to multi-view image generation.

The chapter begins by introducing the fundamental task of Novel View
Synthesis (NVS) and its significance. We then delve into traditional
3D reconstruction techniques (Section \ref{sec:3d-reconstruction})
and their inherent limitations, particularly for sparse-input
scenarios, which motivates the exploration of generative methods.
Subsequently, the discussion shifts to the foundational principles of
Deep Generative Models for Image Synthesis, with a focus on diffusion
models (Section \ref{sec:text-to-image}).

Building on this, we explore various methods for Conditioning
Diffusion Models for Enhanced Control and New Tasks (Section
\ref{sec:conditioning-diffusion}), including the crucial role of
camera parameter encoding and the concept of lightweight adaptation
through adapters.

The core of the chapter then examines state-of-the-art
Diffusion-based Multi-View Image Generation techniques (Section
\ref{sec:multi-view-diffusion}), covering both single reference image
novel view synthesis and architectures for coherent multi-view
generation, including specialized multi-view adapters.
%  Finally, the chapter will conclude with a summary of the discussed
% methods and highlight identified research gaps that this thesis
% aims to address.

\section{Traditional 3D Reconstruction Approaches}\label{sec:3d-reconstruction}

Simultaneous Localization and Mapping (SLAM) is a fundamental concept
in the field of computer vision and robotics. It refers to the
process of simultaneously estimating the camera's position and
orientation in a 3D environment while mapping the environment itself.
Originally, SLAM was used to track the position of a robot in a 3D
space, but it has since been applied to a wide range of problems,
including augmented reality, medical applications and novel view
synthesis.

SLAM works by processing sensor data in real-time to create a map of the unknown environment while simultaneously tracking the position of the sensor within that map. This process typically involves several key steps:
\begin{enumerate}
  \item \textbf{Feature Detection}: Identifying distinctive points or features in the environment from sensor data
  \item \textbf{Data Association}: Matching observed features with previously mapped features
  \item \textbf{State Estimation}: Updating the estimated pose of the camera and the map of the environment
  \item \textbf{Loop Closure}: Recognizing when the sensor has returned to a previously visited location and adjusting the map accordingly to reduce accumulated errors
\end{enumerate}

To address these requirements, researchers have
concentrated on creating techniques for machines to independently
build ever more precise scene representations. This integration of
robotics, computer vision, sensor technology, and recent advancements
in artificial intelligence has shaped this field.

Typically, SLAM techniques use a combination of data sources, such as
images, laser range scans, sonars and GPS to effectively map an environment.
In this work I will focus on the use of images for 3D reconstruction.

\subsection{Structure from Motion Pipeline}

One of the most popular methods for 3D reconstruction from images is
COLMAP \cite{colmap}. COLMAP is a general-purpose
structure-from-motion (SfM) \cite{multipleviewgeometry} system that
can automatically reconstruct 3D scenes from a collection of images.
It is a popular choice for 3D reconstruction due to its accuracy,
speed, and ease of use.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{images/related-work/COLMAP.png}
  \caption{COLMAP pipeline}
  \label{fig:colmap-pipeline}
  % https://colmap.github.io/tutorial.html
\end{figure}

Structure from Motion works in two main steps [Figure
\ref{fig:colmap-pipeline}]:
\begin{enumerate}
  \item \textbf{Correspondence Search}: Identify the unique landmarks
    (features) in all of the images and match the same landmarks across images.
  \item \textbf{Incremental Reconstruction}: Estimates the camera
    poses and triangulates 3D points through an iterative process.
\end{enumerate}

First step of correspondence search is feature extraction. COLMAP
uses SIFT \cite{sift} and ORB \cite{orb} methods to extract features
from images. Scale-invariant feature transform (SIFT) is a method for
detecting keypoints that contrast with their surroundings and
describing the local image content around them. It is invariant to
rotation and scale, making it robust for matching features across images taken from different viewpoints and distances. Oriented FAST and Rotated BRIEF (ORB) is a more
computationally efficient alternative to SIFT, combining a high-speed
FAST detector with optimized descriptors for rotation invariance,
offering comparable matching performance with significantly reduced computational requirements.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{images/related-work/feature-matching.jpg}
  \caption{Feature matching result}
  \label{fig:matching-features}
  % https://blog.roboflow.com/image-matching/
\end{figure}

Second step is matching features across images. Feature matching is a
process of finding the best matches of features across the images.
COLMAP uses a variant of the FLANN \cite{flann} library to find the
best matches for each feature. FLANN is a library for performing fast
approximate nearest neighbor searches in high dimensional spaces.
Feature matching is visualized in Figure \ref{fig:matching-features}.

Then the geometric verification step uses the prior knowledge about
the camera model and motion to remove outliers, preparing the
verified feature matches for the subsequent Incremental Reconstruction phase.

When it comes to Incremental Reconstruction, the process is as follows:
\begin{enumerate}
  \item \textbf{Camera Pose Estimation}: Estimate the camera location
    and direction in 3D space for each image.
  \item \textbf{Triangulation}: Triangulate 3D points of the observed
    objects from the camera poses and matched features.
  \item \textbf{Bundle Adjustment}: Refine the camera poses and 3D
    points to minimize the reprojection error.
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{images/related-work/Structure-from-Motion-SfM-process-is-illustrated-The-structure-in-the.png}
  \caption{Camera pose estimation}
  \label{fig:camera-pose-estimation}
  % https://www.researchgate.net/figure/Structure-from-Motion-SfM-process-is-illustrated-The-structure-in-the_fig2_269327935
\end{figure}

The camera pose estimation (visualized in Figure
\ref{fig:camera-pose-estimation}) starts with an initialization step,
where the initial pair of images and matched features between them
are used to estimate the camera pose. Then, the algorithm proceeds to
the loop (Figure \ref{fig:colmap-pipeline}), where the next image is
registered to the reconstruction and the camera pose is estimated
again (triangulation step). After that the bundle adjustment step is
performed to refine the camera poses and 3D points to be consistent
with the entire dataset with images of a given scene. This
optimization process typically uses the Levenberg-Marquardt algorithm
to minimize reprojection error. This process is repeated for each
subsequent batch of images, updating the camera poses and 3D points.

\subsection{Methods for 3D Reconstruction}

The field of 3D reconstruction encompasses various approaches beyond the basic Structure from Motion pipeline. These methods can be broadly categorized into two groups: sparse reconstruction (like SfM) and dense reconstruction methods.

\subsubsection{Dense Reconstruction Methods}

While SfM provides camera poses and a sparse point cloud, dense reconstruction methods aim to create a complete 3D model with detailed surface information. Notable methods include:

\begin{enumerate}
  \item \textbf{Multi-View Stereo (MVS)}: After obtaining camera poses through SfM, MVS algorithms like PMVS \cite{pmvs} generate dense point clouds by matching pixels across multiple images.

  \item \textbf{Depth Map Fusion}: Methods such as COLMAP's MVS pipeline estimate per-image depth maps and then fuse them into a consistent 3D model.

  \item \textbf{Neural Radiance Fields (NeRF)} \cite{nerf}: A more recent approach that represents scenes as continuous 5D functions (spatial location and viewing direction) encoded in neural networks. NeRF takes camera poses from SfM as input and uses ray tracing to synthesize novel views with remarkable detail and view consistency.
\end{enumerate}

\subsubsection{Learning-based Reconstruction}

Recent approaches leverage deep learning for 3D reconstruction, often using SfM-derived data for supervision or initialization:

\begin{enumerate}
  \item \textbf{Learned MVS}: Methods like MVSNet \cite{mvsnet} use convolutional neural networks to learn the depth estimation process directly from images and camera parameters.

  \item \textbf{Single-view Reconstruction}: Networks like Mesh R-CNN \cite{meshrcnn} can estimate 3D structure from a single image by leveraging prior knowledge learned from large datasets.
\end{enumerate}

\subsection{Limitations of 3D Reconstruction}

Structure from Motion is a powerful tool for 3D reconstruction, demonstrating high effectiveness across a variety of scenarios. However, it encounters significant challenges. These include dealing with textureless surfaces, reflective materials, and the computational complexity of processing high-resolution images. Most importantly in the context of this thesis, it struggles with sparse input scenarios, such as those involving a single image or only a few images.

Traditional 3D reconstruction methods like SfM and MVS typically require a dense collection of images with sufficient overlap to establish accurate feature correspondences and camera pose estimations. When faced with limited input views—particularly in the extreme case of a single image—these methods often fail to generate complete and accurate 3D representations. The quality of reconstruction degrades significantly due to:

\begin{enumerate}
  \item \textbf{Geometric ambiguity}: A single image or sparse set of images provides incomplete information about occluded regions and depth, leading to ambiguous geometry.

  \item \textbf{Feature matching limitations}: Fewer images means fewer opportunities to establish reliable feature correspondences across different viewpoints.

  \item \textbf{Inability to triangulate}: Robust triangulation requires features to be visible from multiple viewpoints, which is not possible with very limited inputs.

  \item \textbf{View-dependent effects}: Materials with specular reflections or varying appearance based on viewpoint cannot be accurately modeled without multiple observations.
\end{enumerate}

These limitations have motivated the development of generative approaches to novel view synthesis, particularly using diffusion models trained on large datasets of rendered images of 3D models. Instead of explicitly reconstructing geometry, these methods leverage the power of deep learning to hallucinate plausible views from unseen perspectives.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{images/related-work/instantmesh.png}
  \caption{InstantMesh}
  \label{fig:instantmesh}
\end{figure}

Modern approaches of 3D object generation, such as InstantMesh \cite{instantmesh}, leverage these diffusion models to generate multiple consistent views of an object from minimal input (a single image or even a text prompt). This novel view synthesis is the first step in a pipeline \ref{fig:instantmesh} that can be used to create a 3D model of the object, and it is this particular step that forms the central focus of this thesis.

\section{Deep Generative Models for Image Synthesis}\label{sec:text-to-image}

The emergence of powerful diffusion models has revolutionized the
field of image generation, including novel view synthesis. These
models have demonstrated remarkable capabilities in generating
high-quality images conditioned on various inputs, such as text
prompts, reference images, or camera poses.

\subsection{Diffusion Models}

UNET

\subsection{Text-to-Image Diffusion Models}

Text-to-image diffusion models like Stable Diffusion
\cite{stablediffusion} have shown impressive capabilities in
generating diverse and high-quality images from textual descriptions.
Building upon these foundations, several works have extended these
models to handle image-to-image translation tasks, where a reference
image serves as an additional conditioning signal.

\section{Conditioning Diffusion Models for Enhanced Control and New
Tasks}\label{sec:conditioning-diffusion}

\subsection{Diverse Conditioning Signals}

\subsection{Camera Parameter Encoding for 3D Awareness}

\subsection{Lightweight Adaptation}

To address the limitations of full fine-tuning approaches, recent
works have explored adapter-based methods that allow for more
efficient adaptation of pre-trained models to specific tasks while
preserving their original capabilities.

Adapters are lightweight modules that can be inserted into
pre-trained models to adapt them to new tasks without modifying the
original network parameters. This approach has gained popularity in
natural language processing and has also been applied to diffusion
models for various image generation tasks.

ControlNet \cite{controlnet} introduced a method to add spatial
conditioning to text-to-image diffusion models by training additional
control modules that are connected to the original UNet backbone.
This approach allows for precise control over the generated images
while preserving the original model's capabilities.

Similarly, T2I-Adapter \cite{t2iadapter} proposed a more modular
approach where adapters are trained separately and can be combined to
provide multiple forms of control simultaneously. These methods have
demonstrated the effectiveness of adapter-based approaches for
controlled image generation.

\section{Diffusion-based Multi-View Image
Generation}\label{sec:multi-view-diffusion}

\subsection{Single Reference Image Novel View Synthesis}

Zero-1-to-3 \cite{zero1to3} pioneered the approach of conditioning
diffusion models on both a reference image and camera pose
information to generate novel views. This method demonstrated the
potential of leveraging pre-trained text-to-image models for novel
view synthesis without requiring explicit 3D reconstruction. However,
it often struggles with maintaining geometric consistency across
generated views.

\subsection{Coherent Multi-View Generation Architectures}
% Camera encoding

To address the limitations of single-view approaches, several works
have focused on developing multi-view diffusion models that can
generate multiple consistent views simultaneously.

MVDream \cite{mvdream} extends the self-attention mechanism in
diffusion models to operate across multiple views, enabling the
generation of 3D-consistent images. By jointly modeling multiple
views, this approach significantly improves geometric consistency
compared to methods that generate each view independently.

Similarly, ViewCrafter \cite{viewcrafter} combines video latent
diffusion models \cite{videolatentdiffusion} with 3D point cloud
priors to generate high-fidelity and consistent novel views. By
leveraging the explicit 3D information provided by point clouds and
the generative capabilities of video diffusion models, ViewCrafter
achieves precise control of camera poses and generates high-quality novel views.

CAT3D \cite{cat3d} takes a different approach by simulating a
real-world capture process with a multi-view diffusion model. Given
one or three input images and a set of target novel viewpoints, this
model generates highly consistent novel views that can be used as
input to robust 3D reconstruction techniques.

While these multi-view diffusion models have shown impressive
results, they typically require full fine-tuning of pre-trained
text-to-image models, which is computationally expensive and may lead
to degradation in image quality due to the scarcity of high-quality 3D data.

\subsection{Specialized Multi-View Adapters}

Building upon the success of adapter mechanisms, MV-Adapter
\cite{mvadapter} introduced the first adapter-based solution for
multi-view image generation. Unlike previous approaches that make
invasive modifications to pre-trained text-to-image models and
require full fine-tuning, MV-Adapter enhances these models with a
plug-and-play adapter that preserves the original network structure
and feature space.

MV-Adapter employs a decoupled attention mechanism, where the
original spatial self-attention layers are retained, and new
multi-view attention layers are created by duplicating the structure
and weights of the original layers. These layers are organized in a
parallel architecture, allowing the adapter to inherit the powerful
priors of the pre-trained self-attention layers while efficiently
learning geometric knowledge.

Additionally, MV-Adapter introduces a unified condition encoder that
seamlessly integrates camera parameters and geometric information,
facilitating applications such as text and image-based 3D generation
and texturing. By updating fewer parameters, MV-Adapter enables
efficient training and preserves the prior knowledge embedded in
pre-trained models, mitigating overfitting risks.

problems of the current MVD:
- inconsistant lighting
- poor geometric consistency
- big model training requirements