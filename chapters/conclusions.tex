\Chapter{Conclusions}\label{chapter:conclusions}

This thesis presents a novel approach to diffusion-based novel view synthesis combining FiLM-based camera conditioning with adapter-based training. This chapter summarizes the key findings, discusses limitations, and outlines future research directions.

\section{Summary of Findings}

\begin{enumerate}
  \item \textbf{Hybrid conditioning strategy} combining visual and geometric information shows internal consistency improvements, with optimal configuration ($img\_ref\_scale=1.0$, $cam\_mod\_strength=1.0$) demonstrating complementary information fusion, though overall performance significantly lags behind established methods like Zero123++.

  \item \textbf{Adapter-based training} achieves computational efficiency gains, training only $585M$ parameters ($20\%$ of full model) with $4\times$ faster training time, but this efficiency comes at the cost of synthesis quality, as evidenced by substantial performance gaps in quantitative evaluation.

  \item \textbf{FiLM-based camera conditioning} provides a computationally efficient alternative to complex raymap representations, though the achieved FID scores of 45-50 remain substantially higher than state-of-the-art methods, indicating limited synthesis quality despite artifact reduction compared to raymap approaches.

  \item \textbf{Dual-stream conditioning mechanism} demonstrates architectural feasibility for processing reference images through parallel conditioning streams, though the resulting novel views show significant quality limitations when compared to contemporary approaches.
\end{enumerate}

\section{Limitations and Areas for Improvement}

The experimental results reveal fundamental performance limitations that must be acknowledged:

\textbf{Resolution Constraints}: The current implementation operates at a maximum resolution of $768 \times 768$ pixels, which may be insufficient for applications requiring high-detail novel view synthesis.

\textbf{Single-Object Focus}: The method is primarily designed and evaluated for single-object novel view synthesis. Extension to complex scenes would require architectural modifications and different training strategies.

\textbf{Limited Viewpoint Range}: While the training data covers comprehensive viewpoint distributions 6, 8 or 12 views, extreme viewpoint changes (such as complete 180-degree rotations or significant elevation changes) may still pose challenges for geometric consistency, particularly for objects with complex occlusions or view-dependent materials.

\textbf{Constrained Training Scale and Generalization}: The most significant limitation is the substantial disparity in training data compared to state-of-the-art methods. While the proposed method used 20,000 Objaverse samples, Zero123++ was trained on 800K+ Objaverse models with approximately 10M rendered images - representing a 40$\times$ difference in 3D  model diversity and 100$\times$ difference in training scale. This fundamental data limitation constrains the model's ability to generalize across diverse object types not well-represented in the limited training set, explaining much of the observed performance gap.

\textbf{Inference Speed}: While competitive with similar methods, the 16-second inference time may be prohibitive for real-time applications or interactive systems requiring immediate novel view generation.

\section{Future Research Directions}

Several promising directions emerge from recent research:

\begin{itemize}
  \item \textbf{Advanced architectural foundations}: Adapting the proposed FiLM-based conditioning to modern diffusion transformer architectures (Stable Diffusion 3 \cite{stable_diffusion_3, diffusion_transformers}) could potentially improve synthesis quality through more powerful attention mechanisms, text enrichment, flow matching and scaling properties.

  \item \textbf{Integrated 3D reconstruction pipelines}: Developing hybrid systems that combine explicit 3D reconstruction with generative synthesis could leverage both geometric and generative methods, reconstructing coarse 3D representations then using diffusion models to generate high-quality details such as Human3Diffusion \cite{human3diffusion}.

  \item \textbf{Multi-view consistency and real-time optimization}: Future work could explore stronger geometric constraints for coherent multi-view synthesis and investigate model distillation or quantization for real-time applications in VR/AR and interactive systems.
\end{itemize}

The work presented in this thesis explores the potential of efficient adapter-based approaches for diffusion-based novel view synthesis, though the results highlight significant challenges in achieving competitive performance. While the demonstrated FiLM-based camera conditioning and hybrid adapter architectures offer computational efficiency benefits, the substantial quality gaps compared to established methods—largely attributable to the 40$\times$ smaller training dataset—indicate that efficiency gains are insufficient to compensate for limited training data scale in practical novel view synthesis applications. The systematic experimental framework and performance evaluation provide valuable insights for the research community, particularly in understanding the limitations of adapter-based approaches and the continued need for more sophisticated architectures or training strategies to achieve state-of-the-art synthesis quality.
